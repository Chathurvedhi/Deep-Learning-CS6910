{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS6910 Assignment1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Fashion-MNIST dataset* and scaling input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf     # Only for fashion_mnist dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from sklearn.model_selection import train_test_split # Only for splitting dataset\n",
    "import wandb\n",
    "\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "def plot_image(train_images, train_labels, class_names):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i in range(25):\n",
    "        plt.subplot(5,5,i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
    "        plt.xlabel(class_names[train_labels[i]])\n",
    "    plt.show()\n",
    "\n",
    "plot_image(train_images, train_labels, class_names)\n",
    "\n",
    "# Convert train_images and test_images from (N,28,28) to (N,784)\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "train_images = train_images/ 255\n",
    "test_images = test_images / 255\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of a Layer in the Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feedforward neural network with variable number of hidden layers and neurons with numpy\n",
    "class Layer:\n",
    "    def __init__(self, input_size, output_size, weight_init = \"random\",activation = \"sigmoid\"):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.weight_init = weight_init\n",
    "        self.activation = activation\n",
    "        self.optimizer = None\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        if self.weight_init == \"random\":\n",
    "            self.w = np.random.randn(self.output_size, self.input_size) * 1 / np.sqrt(self.input_size)\n",
    "            self.b = np.random.randn(self.output_size, 1) * 1 / np.sqrt(self.input_size)\n",
    "        elif self.weight_init == \"xavier\":\n",
    "            cap = np.sqrt(6 / (self.input_size + self.output_size))\n",
    "            self.w = np.random.uniform(-cap, cap, (self.output_size, self.input_size))\n",
    "            self.b = np.random.uniform(-cap, cap, (self.output_size, 1))\n",
    "\n",
    "    def forward(self, input):\n",
    "        # input is (input_size, N) \n",
    "        self.input = input\n",
    "        # a = w.X + b, h = activation(a)\n",
    "        self.a = np.dot(self.w, self.input) + self.b\n",
    "        if self.activation == \"sigmoid\":\n",
    "            self.h = 1 / (1 + np.exp(-self.a))\n",
    "        elif self.activation == \"relu\":\n",
    "            self.h = np.maximum(0, self.a)\n",
    "        elif self.activation == \"tanh\":\n",
    "            self.h = np.tanh(self.a)\n",
    "        elif self.activation == \"identity\":\n",
    "            self.h = self.a\n",
    "        #print(self.h.shape)\n",
    "        return self.h\n",
    "    \n",
    "    def backward(self, dh):\n",
    "        # dh is (self.output_size, N)\n",
    "        if self.activation == \"sigmoid\":\n",
    "            da = dh * self.h * (1 - self.h)\n",
    "        elif self.activation == \"relu\":\n",
    "            da = dh * (self.a > 0)\n",
    "        elif self.activation == \"tanh\":\n",
    "            da = dh * (1 - self.h ** 2)\n",
    "        elif self.activation == \"identity\":\n",
    "            da = dh\n",
    "        self.dw = np.dot(da, self.input.T)\n",
    "        # db is (self.output_size, 1) and da is (self.output_size, N)\n",
    "        self.db = np.mean(da, axis = 1, keepdims = True)\n",
    "        self.dx = np.dot(self.w.T, da)      # dx is dh for previous layer\n",
    "        return self.dx\n",
    "    \n",
    "    def update(self):\n",
    "        # different optimizers can be implemented here\n",
    "        self.optimizer.update(self)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of an Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Output_Layer:\n",
    "    def __init__(self, input_size, output_size, weight_init = \"random\"):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.weight_init = weight_init\n",
    "        self.optimizer = None\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        if self.weight_init == \"random\":\n",
    "            self.w = np.random.randn(self.output_size, self.input_size) * 1 / np.sqrt(self.input_size)\n",
    "            self.b = np.random.randn(self.output_size, 1) * 1 / np.sqrt(self.input_size)\n",
    "        elif self.weight_init == \"xavier\":\n",
    "            cap = np.sqrt(6 / (self.input_size + self.output_size))\n",
    "            self.w = np.random.uniform(-cap, cap, (self.output_size, self.input_size))\n",
    "            self.b = np.random.uniform(-cap, cap, (self.output_size, 1))\n",
    "\n",
    "    def forward(self, input):\n",
    "        # input is (input_size, N) \n",
    "        self.input = input\n",
    "        # a = w.X + b, h = activation(a)\n",
    "        self.a = np.dot(self.w, self.input) + self.b\n",
    "        self.h = np.exp(self.a) / np.sum(np.exp(self.a), axis = 0, keepdims = True)\n",
    "        return self.h\n",
    "    \n",
    "    def backward(self, da):\n",
    "        # dh is (self.output_size, N)\n",
    "        self.dw = np.dot(da, self.input.T)\n",
    "        # db is (self.output_size, 1) and da is (self.output_size, N)\n",
    "        self.db = np.mean(da, axis = 1, keepdims = True)\n",
    "        self.dx = np.dot(self.w.T, da)      # dx is dh for previous layer\n",
    "        return self.dx\n",
    "    \n",
    "    def update(self):\n",
    "        # different optimizers can be implemented here\n",
    "        self.optimizer.update(self)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def compute_loss(self, y, y_hat):\n",
    "        # y is (N,) and y_hat is (output_size, N)\n",
    "        y_encode = np.zeros(y_hat.shape)\n",
    "        y_encode[y, range(y_hat.shape[1])] = 1\n",
    "        self.y = y_encode\n",
    "        self.y_hat = y_hat\n",
    "        self.loss = -np.sum(self.y * np.log(self.y_hat)) / self.y.shape[1]\n",
    "        return self.loss\n",
    "    \n",
    "    def compute_grad(self):\n",
    "        # da is (output_size, N)\n",
    "        self.da_out = self.y_hat - self.y\n",
    "        return self.da_out\n",
    "    \n",
    "class MSE:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def compute_loss(self, y, y_hat):\n",
    "        y_encode = np.zeros(y_hat.shape)\n",
    "        y_encode[y, range(y_hat.shape[1])] = 1\n",
    "        self.y = y_encode\n",
    "        self.y_hat = y_hat\n",
    "        self.loss = np.sum((self.y - self.y_hat) ** 2) / 2\n",
    "        return self.loss\n",
    "    \n",
    "    def compute_grad(self):\n",
    "        self.da_out = 2*(self.y_hat - self.y)\n",
    "        return self.da_out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sgd:\n",
    "    def __init__(self, lr, weight_decay = 0):\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def update(self, layer):\n",
    "        layer.w -= self.lr * (layer.dw + self.weight_decay * layer.w)\n",
    "        layer.b -= self.lr * (layer.db + self.weight_decay * layer.b)\n",
    "\n",
    "class momentum:\n",
    "    def __init__(self, lr, weight_decay = 0, beta = 0.9):\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.beta = beta\n",
    "        self.vw = 0\n",
    "        self.vb = 0\n",
    "\n",
    "    def update(self, layer):\n",
    "        self.vw = self.beta * self.vw + self.lr * (layer.dw + self.weight_decay * layer.w)\n",
    "        self.vb = self.beta * self.vb + self.lr * (layer.db + self.weight_decay * layer.b)\n",
    "        layer.w -= self.vw\n",
    "        layer.b -= self.vb\n",
    "\n",
    "class nesterov:\n",
    "    def __init__(self, lr, weight_decay = 0, beta = 0.9):\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.beta = beta\n",
    "        self.vw = 0\n",
    "        self.vb = 0\n",
    "\n",
    "    def update(self, layer):\n",
    "        layer.w = layer.w + self.beta * self.vw\n",
    "        layer.b = layer.b + self.beta * self.vb\n",
    "        self.vw = self.beta * self.vw + self.lr * (layer.dw + self.weight_decay * layer.w)\n",
    "        self.vb = self.beta * self.vb + self.lr * (layer.db + self.weight_decay * layer.b)\n",
    "        layer.w -= self.vw + self.beta * self.vw\n",
    "        layer.b -= self.vb + self.beta * self.vb\n",
    "\n",
    "class rmsprop:\n",
    "    def __init__(self, lr, weight_decay = 0, beta = 0.9, epsilon = 1e-8):\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.beta = beta\n",
    "        self.epsilon = epsilon\n",
    "        self.sw = 0\n",
    "        self.sb = 0\n",
    "\n",
    "    def update(self, layer):\n",
    "        self.sw = self.beta * self.sw + (1 - self.beta) * layer.dw ** 2\n",
    "        self.sb = self.beta * self.sb + (1 - self.beta) * layer.db ** 2\n",
    "        layer.w -= self.lr * layer.dw / (np.sqrt(self.sw) + self.epsilon) + self.lr * self.weight_decay * layer.w\n",
    "        layer.b -= self.lr * layer.db / (np.sqrt(self.sb) + self.epsilon) + self.lr * self.weight_decay * layer.b\n",
    "\n",
    "class adam:\n",
    "    def __init__(self, lr, weight_decay = 0, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8):\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.vw = 0\n",
    "        self.vb = 0\n",
    "        self.sw = 0\n",
    "        self.sb = 0\n",
    "        self.t = 0\n",
    "\n",
    "    def update(self, layer):\n",
    "        self.t += 1\n",
    "        self.vw = self.beta1 * self.vw + (1 - self.beta1) * layer.dw\n",
    "        self.vb = self.beta1 * self.vb + (1 - self.beta1) * layer.db\n",
    "        self.sw = self.beta2 * self.sw + (1 - self.beta2) * layer.dw ** 2\n",
    "        self.sb = self.beta2 * self.sb + (1 - self.beta2) * layer.db ** 2\n",
    "        vw_temp = self.vw / (1 - np.power(self.beta1, self.t))\n",
    "        vb_temp = self.vb / (1 - np.power(self.beta1, self.t))\n",
    "        sw_temp = self.sw / (1 - np.power(self.beta2, self.t))\n",
    "        sb_temp = self.sb / (1 - np.power(self.beta2, self.t))\n",
    "        layer.w -= self.lr * vw_temp / (np.sqrt(sw_temp) + self.epsilon) + self.lr * self.weight_decay * layer.w\n",
    "        layer.b -= self.lr * vb_temp / (np.sqrt(sb_temp) + self.epsilon) + self.lr * self.weight_decay * layer.b\n",
    "\n",
    "class nadam:\n",
    "    def __init__(self, lr, weight_decay = 0, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8):\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.vw = 0\n",
    "        self.vb = 0\n",
    "        self.sw = 0\n",
    "        self.sb = 0\n",
    "        self.t = 0\n",
    "\n",
    "    def update(self, layer):\n",
    "        self.t += 1\n",
    "        self.vw = self.beta1 * self.vw + (1 - self.beta1) * layer.dw\n",
    "        self.vb = self.beta1 * self.vb + (1 - self.beta1) * layer.db\n",
    "        self.sw = self.beta2 * self.sw + (1 - self.beta2) * layer.dw ** 2\n",
    "        self.sb = self.beta2 * self.sb + (1 - self.beta2) * layer.db ** 2\n",
    "        vw_temp = self.vw / (1 - np.power(self.beta1, self.t))\n",
    "        vb_temp = self.vb / (1 - np.power(self.beta1, self.t))\n",
    "        sw_temp = self.sw / (1 - np.power(self.beta2, self.t))\n",
    "        sb_temp = self.sb / (1 - np.power(self.beta2, self.t))\n",
    "        layer.w -= self.lr * (self.beta1 * vw_temp + (1 - self.beta1) * layer.dw / (1 - np.power(self.beta1, self.t))) / (np.sqrt(sw_temp + self.epsilon)) + self.lr * self.weight_decay * layer.w\n",
    "        layer.b -= self.lr * (self.beta1 * vb_temp + (1 - self.beta1) * layer.db / (1 - np.power(self.beta1, self.t))) / (np.sqrt(sb_temp + self.epsilon)) + self.lr * self.weight_decay * layer.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network:\n",
    "    def __init__(self, hidden_layers, layer_size, activation, loss, optimizer, batch_size = 1, epochs = 100, weight_init = \"xavier\"):\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.layer_size = layer_size\n",
    "        self.activation = activation\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.batch_size = batch_size\n",
    "        self.weight_init = weight_init\n",
    "        self.epochs = epochs\n",
    "        self.layers = []\n",
    "        self.train_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_losses = []\n",
    "        self.val_accuracies = []\n",
    "        self.init_layers()\n",
    "\n",
    "\n",
    "    def init_layers(self):\n",
    "        # Input Layer using Layer class\n",
    "        L = Layer(28*28, self.layer_size, self.weight_init, self.activation)\n",
    "        L.optimizer = deepcopy(self.optimizer)\n",
    "        self.layers.append(L)\n",
    "        # Hidden Layers using Layer class\n",
    "        for i in range(self.hidden_layers-1):\n",
    "            L = Layer(self.layer_size, self.layer_size, self.weight_init, self.activation)\n",
    "            L.optimizer = deepcopy(self.optimizer)\n",
    "            self.layers.append(L)\n",
    "        # Output Layer using Layer class\n",
    "        L = Output_Layer(self.layer_size, 10, self.weight_init)\n",
    "        L.optimizer = deepcopy(self.optimizer)\n",
    "        self.layers.append(L)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is (28*28, N)\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i].forward(x)\n",
    "        self.y_end = x\n",
    "        # x is (10, N)\n",
    "        return x\n",
    "        \n",
    "    \n",
    "    def backward(self, y):\n",
    "        grad = self.loss.compute_grad()\n",
    "        for i in range(len(self.layers)-1, -1, -1):\n",
    "            grad = self.layers[i].backward(grad)\n",
    "        return grad\n",
    "    \n",
    "    def update(self):\n",
    "        for i in range(len(self.layers)):\n",
    "            self.layers[i].update()\n",
    "\n",
    "    def accuracy(self, y, y_hat):\n",
    "        y_hat = np.argmax(y_hat, axis = 0)\n",
    "        return np.sum(y_hat == y) / len(y)\n",
    "    \n",
    "    def create_batches(self, X, y):\n",
    "        # X is (N, 28*28) and y is (N,)\n",
    "        batches = []\n",
    "        for i in range(len(y) // self.batch_size):\n",
    "            s = i * self.batch_size\n",
    "            e = (i+1) * self.batch_size\n",
    "            batches.append((X[s:e], y[s:e]))\n",
    "        if len(y) % self.batch_size != 0:\n",
    "            batches.append((X[e:], y[e:]))\n",
    "        return batches\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.1)\n",
    "        train_batches = self.create_batches(X_train, y_train)\n",
    "        val_batches = self.create_batches(X_val, y_val)\n",
    "        num_train_batches = len(train_batches)\n",
    "        num_val_batches = len(val_batches)\n",
    "        for ep in range(1, self.epochs + 1):\n",
    "            train_loss = 0\n",
    "            train_acc = 0\n",
    "            val_loss = 0\n",
    "            val_acc = 0\n",
    "\n",
    "            for X_run, y_run in train_batches:\n",
    "                X_run = X_run.T\n",
    "                y_run_hat = self.forward(X_run)\n",
    "                #print(y_run_hat.shape, y_run.shape)\n",
    "                train_loss += self.loss.compute_loss(y_run, y_run_hat)\n",
    "                train_acc += self.accuracy(y_run, y_run_hat)\n",
    "                self.backward(y_run)\n",
    "                self.update()\n",
    "\n",
    "            train_loss /= num_train_batches\n",
    "            train_acc /= num_train_batches\n",
    "\n",
    "            for X_runv, y_runv in val_batches:\n",
    "                X_runv = X_runv.T\n",
    "                y_runv_hat = self.forward(X_runv)\n",
    "                val_loss += self.loss.compute_loss(y_runv, y_runv_hat)\n",
    "                val_acc += self.accuracy(y_runv, y_runv_hat)\n",
    "                \n",
    "            val_loss /= num_val_batches\n",
    "            val_acc /= num_val_batches\n",
    "\n",
    "            wandb.log({\"Epoch\": ep, \"Train Loss\": train_loss, \"Train Accuracy\": train_acc, \"Val Loss\": val_loss, \"Val Accuracy\": val_acc})\n",
    "            print(f\"Epoch: {ep} Train Loss: {train_loss} Train Acc: {train_acc} Val Loss: {val_loss} Val Acc: {val_acc}\")\n",
    "\n",
    "            self.val_losses.append(val_loss)\n",
    "            self.val_accuracies.append(val_acc)\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.train_accuracies.append(train_acc)\n",
    "\n",
    "        print(\"Model trained :)\")\n",
    "\n",
    "    def eval_test(self, X, y):\n",
    "        X = X.T\n",
    "        y_hat = self.forward(X)\n",
    "        test_loss = self.loss.compute_loss(y, y_hat)\n",
    "        test_acc = self.accuracy(y, y_hat)\n",
    "        return test_acc\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test run without wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_loss = CrossEntropy()\n",
    "m_opt = nadam(0.001)\n",
    "\n",
    "sweep_configuration = {\n",
    "    'method': 'random',\n",
    "    'metric': {'goal': 'maximize', 'name': 'test_acc'},\n",
    "    'parameters': \n",
    "    {\n",
    "        'hidden_layers': {'values': [2, 3]},\n",
    "        'layer_size': {'values': [128, 256]},\n",
    "        'activation': {'values': ['relu', 'tanh']},\n",
    "        'loss': {'values': ['cross_entropy']},\n",
    "        'optimizer': {'values': ['adam', 'nadam']},\n",
    "        'batch_size': {'values': [32, 64, 128]},\n",
    "        'weight_init': {'values': ['xavier']},\n",
    "        'epochs': {'values': [5, 10, 20, 30, 40]},\n",
    "        'learning_rate': {'values': [0.01, 0.001, 0.0001, 0.00001]},\n",
    "        'weight_decay' : {'values': [1e-4, 1e-5, 1e-6, 0]},\n",
    "        'beta1': {'values': [0.9]},\n",
    "        'beta2': {'values': [0.999]},\n",
    "        'epsilon': {'values': [1e-8, 1e-9, 1e-10]},\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wandb sweeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the W&B Python Library and log into W&B\n",
    "# 7f43f605c6b6d8cfd614fa05cd9da37bd2b0ddda\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "# 1: Define objective/training function\n",
    "def objective(config):\n",
    "    loss = CrossEntropy()\n",
    "    if config.optimizer == 'sgd':\n",
    "        optimizer = sgd(config.learning_rate)\n",
    "    elif config.optimizer == 'momentum':\n",
    "        optimizer = momentum(config.learning_rate, config.weight_decay, config.beta1)\n",
    "    elif config.optimizer == 'nesterov':\n",
    "        optimizer = nesterov(config.learning_rate, config.weight_decay, config.beta1)\n",
    "    elif config.optimizer == 'rmsprop':\n",
    "        optimizer = rmsprop(config.learning_rate, config.weight_decay, config.beta1, config.epsilon)\n",
    "    elif config.optimizer == 'adam':\n",
    "        optimizer = adam(config.learning_rate, config.weight_decay, config.beta1, config.beta2, config.epsilon)\n",
    "    elif config.optimizer == 'nadam':\n",
    "        optimizer = nadam(config.learning_rate, config.weight_decay, config.beta1, config.beta2, config.epsilon)\n",
    "    model = Neural_Network(hidden_layers=config.hidden_layers, layer_size=config.layer_size, activation=config.activation, loss=loss, optimizer=optimizer, batch_size=config.batch_size, weight_init=config.weight_init, epochs=config.epochs)\n",
    "    model.train(train_images, train_labels)\n",
    "    test_acc_final = model.eval_test(test_images, test_labels)\n",
    "    wandb.log({'test_acc': test_acc_final})\n",
    "\n",
    "def main():\n",
    "    wandb.init(project='CS20B021_A1')\n",
    "    objective(wandb.config)\n",
    "    \n",
    "\n",
    "\n",
    "# 2: Define the search space\n",
    "\n",
    "# 3: Start the sweep\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project='CS20B021_A1')\n",
    "wandb.agent(sweep_id, function=main, count=150)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
